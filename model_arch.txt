Parameter: input_process.poseEmbedding.weight, requires_grad: False, shape: torch.Size([512, 263])
Parameter: input_process.poseEmbedding.bias, requires_grad: False, shape: torch.Size([512])
Parameter: prior_network.position_embeddings, requires_grad: True, shape: torch.Size([10, 512])
Parameter: prior_network.encoder_layers.self_attn.in_proj_weight, requires_grad: True, shape: torch.Size([1536, 512])
Parameter: prior_network.encoder_layers.self_attn.in_proj_bias, requires_grad: True, shape: torch.Size([1536])
Parameter: prior_network.encoder_layers.self_attn.out_proj.weight, requires_grad: True, shape: torch.Size([512, 512])
Parameter: prior_network.encoder_layers.self_attn.out_proj.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.encoder_layers.linear1.weight, requires_grad: True, shape: torch.Size([256, 512])
Parameter: prior_network.encoder_layers.linear1.bias, requires_grad: True, shape: torch.Size([256])
Parameter: prior_network.encoder_layers.linear2.weight, requires_grad: True, shape: torch.Size([512, 256])
Parameter: prior_network.encoder_layers.linear2.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.encoder_layers.norm1.weight, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.encoder_layers.norm1.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.encoder_layers.norm2.weight, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.encoder_layers.norm2.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.0.self_attn.in_proj_weight, requires_grad: True, shape: torch.Size([1536, 512])
Parameter: prior_network.transformer_encoder.layers.0.self_attn.in_proj_bias, requires_grad: True, shape: torch.Size([1536])
Parameter: prior_network.transformer_encoder.layers.0.self_attn.out_proj.weight, requires_grad: True, shape: torch.Size([512, 512])
Parameter: prior_network.transformer_encoder.layers.0.self_attn.out_proj.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.0.linear1.weight, requires_grad: True, shape: torch.Size([256, 512])
Parameter: prior_network.transformer_encoder.layers.0.linear1.bias, requires_grad: True, shape: torch.Size([256])
Parameter: prior_network.transformer_encoder.layers.0.linear2.weight, requires_grad: True, shape: torch.Size([512, 256])
Parameter: prior_network.transformer_encoder.layers.0.linear2.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.0.norm1.weight, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.0.norm1.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.0.norm2.weight, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.0.norm2.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.1.self_attn.in_proj_weight, requires_grad: True, shape: torch.Size([1536, 512])
Parameter: prior_network.transformer_encoder.layers.1.self_attn.in_proj_bias, requires_grad: True, shape: torch.Size([1536])
Parameter: prior_network.transformer_encoder.layers.1.self_attn.out_proj.weight, requires_grad: True, shape: torch.Size([512, 512])
Parameter: prior_network.transformer_encoder.layers.1.self_attn.out_proj.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.1.linear1.weight, requires_grad: True, shape: torch.Size([256, 512])
Parameter: prior_network.transformer_encoder.layers.1.linear1.bias, requires_grad: True, shape: torch.Size([256])
Parameter: prior_network.transformer_encoder.layers.1.linear2.weight, requires_grad: True, shape: torch.Size([512, 256])
Parameter: prior_network.transformer_encoder.layers.1.linear2.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.1.norm1.weight, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.1.norm1.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.1.norm2.weight, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.1.norm2.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.2.self_attn.in_proj_weight, requires_grad: True, shape: torch.Size([1536, 512])
Parameter: prior_network.transformer_encoder.layers.2.self_attn.in_proj_bias, requires_grad: True, shape: torch.Size([1536])
Parameter: prior_network.transformer_encoder.layers.2.self_attn.out_proj.weight, requires_grad: True, shape: torch.Size([512, 512])
Parameter: prior_network.transformer_encoder.layers.2.self_attn.out_proj.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.2.linear1.weight, requires_grad: True, shape: torch.Size([256, 512])
Parameter: prior_network.transformer_encoder.layers.2.linear1.bias, requires_grad: True, shape: torch.Size([256])
Parameter: prior_network.transformer_encoder.layers.2.linear2.weight, requires_grad: True, shape: torch.Size([512, 256])
Parameter: prior_network.transformer_encoder.layers.2.linear2.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.2.norm1.weight, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.2.norm1.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.2.norm2.weight, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.2.norm2.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.3.self_attn.in_proj_weight, requires_grad: True, shape: torch.Size([1536, 512])
Parameter: prior_network.transformer_encoder.layers.3.self_attn.in_proj_bias, requires_grad: True, shape: torch.Size([1536])
Parameter: prior_network.transformer_encoder.layers.3.self_attn.out_proj.weight, requires_grad: True, shape: torch.Size([512, 512])
Parameter: prior_network.transformer_encoder.layers.3.self_attn.out_proj.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.3.linear1.weight, requires_grad: True, shape: torch.Size([256, 512])
Parameter: prior_network.transformer_encoder.layers.3.linear1.bias, requires_grad: True, shape: torch.Size([256])
Parameter: prior_network.transformer_encoder.layers.3.linear2.weight, requires_grad: True, shape: torch.Size([512, 256])
Parameter: prior_network.transformer_encoder.layers.3.linear2.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.3.norm1.weight, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.3.norm1.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.3.norm2.weight, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.transformer_encoder.layers.3.norm2.bias, requires_grad: True, shape: torch.Size([512])
Parameter: prior_network.output_layer.weight, requires_grad: True, shape: torch.Size([512, 512])
Parameter: prior_network.output_layer.bias, requires_grad: True, shape: torch.Size([512])
Parameter: mlp_fc.atomic_action_mlp.fc1.weight, requires_grad: False, shape: torch.Size([256, 512])
Parameter: mlp_fc.atomic_action_mlp.fc1.bias, requires_grad: False, shape: torch.Size([256])
Parameter: mlp_fc.atomic_action_mlp.fc2.weight, requires_grad: False, shape: torch.Size([256, 256])
Parameter: mlp_fc.atomic_action_mlp.fc2.bias, requires_grad: False, shape: torch.Size([256])
Parameter: mlp_fc.atomic_action_mlp.fc3.weight, requires_grad: False, shape: torch.Size([256, 256])
Parameter: mlp_fc.atomic_action_mlp.fc3.bias, requires_grad: False, shape: torch.Size([256])
Parameter: mlp_fc.detail_description_mlp.fc1.weight, requires_grad: True, shape: torch.Size([256, 512])
Parameter: mlp_fc.detail_description_mlp.fc1.bias, requires_grad: True, shape: torch.Size([256])
Parameter: mlp_fc.detail_description_mlp.fc2.weight, requires_grad: True, shape: torch.Size([256, 256])
Parameter: mlp_fc.detail_description_mlp.fc2.bias, requires_grad: True, shape: torch.Size([256])
Parameter: mlp_fc.detail_description_mlp.fc3.weight, requires_grad: True, shape: torch.Size([256, 256])
Parameter: mlp_fc.detail_description_mlp.fc3.bias, requires_grad: True, shape: torch.Size([256])
Parameter: seqTransEncoder.layers.0.self_attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: seqTransEncoder.layers.0.self_attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: seqTransEncoder.layers.0.self_attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: seqTransEncoder.layers.0.self_attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.0.linear1.weight, requires_grad: False, shape: torch.Size([1024, 512])
Parameter: seqTransEncoder.layers.0.linear1.bias, requires_grad: False, shape: torch.Size([1024])
Parameter: seqTransEncoder.layers.0.linear2.weight, requires_grad: False, shape: torch.Size([512, 1024])
Parameter: seqTransEncoder.layers.0.linear2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.0.norm1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.0.norm1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.0.norm2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.0.norm2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.1.self_attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: seqTransEncoder.layers.1.self_attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: seqTransEncoder.layers.1.self_attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: seqTransEncoder.layers.1.self_attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.1.linear1.weight, requires_grad: False, shape: torch.Size([1024, 512])
Parameter: seqTransEncoder.layers.1.linear1.bias, requires_grad: False, shape: torch.Size([1024])
Parameter: seqTransEncoder.layers.1.linear2.weight, requires_grad: False, shape: torch.Size([512, 1024])
Parameter: seqTransEncoder.layers.1.linear2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.1.norm1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.1.norm1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.1.norm2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.1.norm2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.2.self_attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: seqTransEncoder.layers.2.self_attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: seqTransEncoder.layers.2.self_attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: seqTransEncoder.layers.2.self_attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.2.linear1.weight, requires_grad: False, shape: torch.Size([1024, 512])
Parameter: seqTransEncoder.layers.2.linear1.bias, requires_grad: False, shape: torch.Size([1024])
Parameter: seqTransEncoder.layers.2.linear2.weight, requires_grad: False, shape: torch.Size([512, 1024])
Parameter: seqTransEncoder.layers.2.linear2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.2.norm1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.2.norm1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.2.norm2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.2.norm2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.3.self_attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: seqTransEncoder.layers.3.self_attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: seqTransEncoder.layers.3.self_attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: seqTransEncoder.layers.3.self_attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.3.linear1.weight, requires_grad: False, shape: torch.Size([1024, 512])
Parameter: seqTransEncoder.layers.3.linear1.bias, requires_grad: False, shape: torch.Size([1024])
Parameter: seqTransEncoder.layers.3.linear2.weight, requires_grad: False, shape: torch.Size([512, 1024])
Parameter: seqTransEncoder.layers.3.linear2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.3.norm1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.3.norm1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.3.norm2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.3.norm2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.4.self_attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: seqTransEncoder.layers.4.self_attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: seqTransEncoder.layers.4.self_attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: seqTransEncoder.layers.4.self_attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.4.linear1.weight, requires_grad: False, shape: torch.Size([1024, 512])
Parameter: seqTransEncoder.layers.4.linear1.bias, requires_grad: False, shape: torch.Size([1024])
Parameter: seqTransEncoder.layers.4.linear2.weight, requires_grad: False, shape: torch.Size([512, 1024])
Parameter: seqTransEncoder.layers.4.linear2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.4.norm1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.4.norm1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.4.norm2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.4.norm2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.5.self_attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: seqTransEncoder.layers.5.self_attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: seqTransEncoder.layers.5.self_attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: seqTransEncoder.layers.5.self_attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.5.linear1.weight, requires_grad: False, shape: torch.Size([1024, 512])
Parameter: seqTransEncoder.layers.5.linear1.bias, requires_grad: False, shape: torch.Size([1024])
Parameter: seqTransEncoder.layers.5.linear2.weight, requires_grad: False, shape: torch.Size([512, 1024])
Parameter: seqTransEncoder.layers.5.linear2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.5.norm1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.5.norm1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.5.norm2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.5.norm2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.6.self_attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: seqTransEncoder.layers.6.self_attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: seqTransEncoder.layers.6.self_attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: seqTransEncoder.layers.6.self_attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.6.linear1.weight, requires_grad: False, shape: torch.Size([1024, 512])
Parameter: seqTransEncoder.layers.6.linear1.bias, requires_grad: False, shape: torch.Size([1024])
Parameter: seqTransEncoder.layers.6.linear2.weight, requires_grad: False, shape: torch.Size([512, 1024])
Parameter: seqTransEncoder.layers.6.linear2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.6.norm1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.6.norm1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.6.norm2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.6.norm2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.7.self_attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: seqTransEncoder.layers.7.self_attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: seqTransEncoder.layers.7.self_attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: seqTransEncoder.layers.7.self_attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.7.linear1.weight, requires_grad: False, shape: torch.Size([1024, 512])
Parameter: seqTransEncoder.layers.7.linear1.bias, requires_grad: False, shape: torch.Size([1024])
Parameter: seqTransEncoder.layers.7.linear2.weight, requires_grad: False, shape: torch.Size([512, 1024])
Parameter: seqTransEncoder.layers.7.linear2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.7.norm1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.7.norm1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.7.norm2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: seqTransEncoder.layers.7.norm2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: embed_timestep.time_embed.0.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: embed_timestep.time_embed.0.bias, requires_grad: False, shape: torch.Size([512])
Parameter: embed_timestep.time_embed.2.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: embed_timestep.time_embed.2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: embed_text.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: embed_text.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.positional_embedding, requires_grad: False, shape: torch.Size([77, 512])
Parameter: clip_model.text_projection, requires_grad: False, shape: torch.Size([512, 512])
Parameter: clip_model.logit_scale, requires_grad: False, shape: torch.Size([])
Parameter: clip_model.visual.class_embedding, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.positional_embedding, requires_grad: False, shape: torch.Size([50, 768])
Parameter: clip_model.visual.proj, requires_grad: False, shape: torch.Size([768, 512])
Parameter: clip_model.visual.conv1.weight, requires_grad: False, shape: torch.Size([768, 3, 32, 32])
Parameter: clip_model.visual.ln_pre.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.ln_pre.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.0.attn.in_proj_weight, requires_grad: False, shape: torch.Size([2304, 768])
Parameter: clip_model.visual.transformer.resblocks.0.attn.in_proj_bias, requires_grad: False, shape: torch.Size([2304])
Parameter: clip_model.visual.transformer.resblocks.0.attn.out_proj.weight, requires_grad: False, shape: torch.Size([768, 768])
Parameter: clip_model.visual.transformer.resblocks.0.attn.out_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.0.ln_1.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.0.ln_1.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.0.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([3072, 768])
Parameter: clip_model.visual.transformer.resblocks.0.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([3072])
Parameter: clip_model.visual.transformer.resblocks.0.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([768, 3072])
Parameter: clip_model.visual.transformer.resblocks.0.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.0.ln_2.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.0.ln_2.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.1.attn.in_proj_weight, requires_grad: False, shape: torch.Size([2304, 768])
Parameter: clip_model.visual.transformer.resblocks.1.attn.in_proj_bias, requires_grad: False, shape: torch.Size([2304])
Parameter: clip_model.visual.transformer.resblocks.1.attn.out_proj.weight, requires_grad: False, shape: torch.Size([768, 768])
Parameter: clip_model.visual.transformer.resblocks.1.attn.out_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.1.ln_1.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.1.ln_1.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.1.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([3072, 768])
Parameter: clip_model.visual.transformer.resblocks.1.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([3072])
Parameter: clip_model.visual.transformer.resblocks.1.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([768, 3072])
Parameter: clip_model.visual.transformer.resblocks.1.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.1.ln_2.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.1.ln_2.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.2.attn.in_proj_weight, requires_grad: False, shape: torch.Size([2304, 768])
Parameter: clip_model.visual.transformer.resblocks.2.attn.in_proj_bias, requires_grad: False, shape: torch.Size([2304])
Parameter: clip_model.visual.transformer.resblocks.2.attn.out_proj.weight, requires_grad: False, shape: torch.Size([768, 768])
Parameter: clip_model.visual.transformer.resblocks.2.attn.out_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.2.ln_1.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.2.ln_1.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.2.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([3072, 768])
Parameter: clip_model.visual.transformer.resblocks.2.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([3072])
Parameter: clip_model.visual.transformer.resblocks.2.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([768, 3072])
Parameter: clip_model.visual.transformer.resblocks.2.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.2.ln_2.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.2.ln_2.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.3.attn.in_proj_weight, requires_grad: False, shape: torch.Size([2304, 768])
Parameter: clip_model.visual.transformer.resblocks.3.attn.in_proj_bias, requires_grad: False, shape: torch.Size([2304])
Parameter: clip_model.visual.transformer.resblocks.3.attn.out_proj.weight, requires_grad: False, shape: torch.Size([768, 768])
Parameter: clip_model.visual.transformer.resblocks.3.attn.out_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.3.ln_1.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.3.ln_1.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.3.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([3072, 768])
Parameter: clip_model.visual.transformer.resblocks.3.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([3072])
Parameter: clip_model.visual.transformer.resblocks.3.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([768, 3072])
Parameter: clip_model.visual.transformer.resblocks.3.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.3.ln_2.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.3.ln_2.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.4.attn.in_proj_weight, requires_grad: False, shape: torch.Size([2304, 768])
Parameter: clip_model.visual.transformer.resblocks.4.attn.in_proj_bias, requires_grad: False, shape: torch.Size([2304])
Parameter: clip_model.visual.transformer.resblocks.4.attn.out_proj.weight, requires_grad: False, shape: torch.Size([768, 768])
Parameter: clip_model.visual.transformer.resblocks.4.attn.out_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.4.ln_1.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.4.ln_1.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.4.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([3072, 768])
Parameter: clip_model.visual.transformer.resblocks.4.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([3072])
Parameter: clip_model.visual.transformer.resblocks.4.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([768, 3072])
Parameter: clip_model.visual.transformer.resblocks.4.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.4.ln_2.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.4.ln_2.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.5.attn.in_proj_weight, requires_grad: False, shape: torch.Size([2304, 768])
Parameter: clip_model.visual.transformer.resblocks.5.attn.in_proj_bias, requires_grad: False, shape: torch.Size([2304])
Parameter: clip_model.visual.transformer.resblocks.5.attn.out_proj.weight, requires_grad: False, shape: torch.Size([768, 768])
Parameter: clip_model.visual.transformer.resblocks.5.attn.out_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.5.ln_1.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.5.ln_1.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.5.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([3072, 768])
Parameter: clip_model.visual.transformer.resblocks.5.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([3072])
Parameter: clip_model.visual.transformer.resblocks.5.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([768, 3072])
Parameter: clip_model.visual.transformer.resblocks.5.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.5.ln_2.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.5.ln_2.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.6.attn.in_proj_weight, requires_grad: False, shape: torch.Size([2304, 768])
Parameter: clip_model.visual.transformer.resblocks.6.attn.in_proj_bias, requires_grad: False, shape: torch.Size([2304])
Parameter: clip_model.visual.transformer.resblocks.6.attn.out_proj.weight, requires_grad: False, shape: torch.Size([768, 768])
Parameter: clip_model.visual.transformer.resblocks.6.attn.out_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.6.ln_1.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.6.ln_1.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.6.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([3072, 768])
Parameter: clip_model.visual.transformer.resblocks.6.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([3072])
Parameter: clip_model.visual.transformer.resblocks.6.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([768, 3072])
Parameter: clip_model.visual.transformer.resblocks.6.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.6.ln_2.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.6.ln_2.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.7.attn.in_proj_weight, requires_grad: False, shape: torch.Size([2304, 768])
Parameter: clip_model.visual.transformer.resblocks.7.attn.in_proj_bias, requires_grad: False, shape: torch.Size([2304])
Parameter: clip_model.visual.transformer.resblocks.7.attn.out_proj.weight, requires_grad: False, shape: torch.Size([768, 768])
Parameter: clip_model.visual.transformer.resblocks.7.attn.out_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.7.ln_1.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.7.ln_1.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.7.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([3072, 768])
Parameter: clip_model.visual.transformer.resblocks.7.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([3072])
Parameter: clip_model.visual.transformer.resblocks.7.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([768, 3072])
Parameter: clip_model.visual.transformer.resblocks.7.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.7.ln_2.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.7.ln_2.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.8.attn.in_proj_weight, requires_grad: False, shape: torch.Size([2304, 768])
Parameter: clip_model.visual.transformer.resblocks.8.attn.in_proj_bias, requires_grad: False, shape: torch.Size([2304])
Parameter: clip_model.visual.transformer.resblocks.8.attn.out_proj.weight, requires_grad: False, shape: torch.Size([768, 768])
Parameter: clip_model.visual.transformer.resblocks.8.attn.out_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.8.ln_1.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.8.ln_1.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.8.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([3072, 768])
Parameter: clip_model.visual.transformer.resblocks.8.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([3072])
Parameter: clip_model.visual.transformer.resblocks.8.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([768, 3072])
Parameter: clip_model.visual.transformer.resblocks.8.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.8.ln_2.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.8.ln_2.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.9.attn.in_proj_weight, requires_grad: False, shape: torch.Size([2304, 768])
Parameter: clip_model.visual.transformer.resblocks.9.attn.in_proj_bias, requires_grad: False, shape: torch.Size([2304])
Parameter: clip_model.visual.transformer.resblocks.9.attn.out_proj.weight, requires_grad: False, shape: torch.Size([768, 768])
Parameter: clip_model.visual.transformer.resblocks.9.attn.out_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.9.ln_1.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.9.ln_1.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.9.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([3072, 768])
Parameter: clip_model.visual.transformer.resblocks.9.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([3072])
Parameter: clip_model.visual.transformer.resblocks.9.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([768, 3072])
Parameter: clip_model.visual.transformer.resblocks.9.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.9.ln_2.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.9.ln_2.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.10.attn.in_proj_weight, requires_grad: False, shape: torch.Size([2304, 768])
Parameter: clip_model.visual.transformer.resblocks.10.attn.in_proj_bias, requires_grad: False, shape: torch.Size([2304])
Parameter: clip_model.visual.transformer.resblocks.10.attn.out_proj.weight, requires_grad: False, shape: torch.Size([768, 768])
Parameter: clip_model.visual.transformer.resblocks.10.attn.out_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.10.ln_1.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.10.ln_1.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.10.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([3072, 768])
Parameter: clip_model.visual.transformer.resblocks.10.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([3072])
Parameter: clip_model.visual.transformer.resblocks.10.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([768, 3072])
Parameter: clip_model.visual.transformer.resblocks.10.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.10.ln_2.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.10.ln_2.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.11.attn.in_proj_weight, requires_grad: False, shape: torch.Size([2304, 768])
Parameter: clip_model.visual.transformer.resblocks.11.attn.in_proj_bias, requires_grad: False, shape: torch.Size([2304])
Parameter: clip_model.visual.transformer.resblocks.11.attn.out_proj.weight, requires_grad: False, shape: torch.Size([768, 768])
Parameter: clip_model.visual.transformer.resblocks.11.attn.out_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.11.ln_1.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.11.ln_1.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.11.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([3072, 768])
Parameter: clip_model.visual.transformer.resblocks.11.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([3072])
Parameter: clip_model.visual.transformer.resblocks.11.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([768, 3072])
Parameter: clip_model.visual.transformer.resblocks.11.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.11.ln_2.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.transformer.resblocks.11.ln_2.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.ln_post.weight, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.visual.ln_post.bias, requires_grad: False, shape: torch.Size([768])
Parameter: clip_model.transformer.resblocks.0.attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: clip_model.transformer.resblocks.0.attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: clip_model.transformer.resblocks.0.attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: clip_model.transformer.resblocks.0.attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.0.ln_1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.0.ln_1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.0.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([2048, 512])
Parameter: clip_model.transformer.resblocks.0.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([2048])
Parameter: clip_model.transformer.resblocks.0.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([512, 2048])
Parameter: clip_model.transformer.resblocks.0.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.0.ln_2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.0.ln_2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.1.attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: clip_model.transformer.resblocks.1.attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: clip_model.transformer.resblocks.1.attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: clip_model.transformer.resblocks.1.attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.1.ln_1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.1.ln_1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.1.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([2048, 512])
Parameter: clip_model.transformer.resblocks.1.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([2048])
Parameter: clip_model.transformer.resblocks.1.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([512, 2048])
Parameter: clip_model.transformer.resblocks.1.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.1.ln_2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.1.ln_2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.2.attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: clip_model.transformer.resblocks.2.attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: clip_model.transformer.resblocks.2.attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: clip_model.transformer.resblocks.2.attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.2.ln_1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.2.ln_1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.2.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([2048, 512])
Parameter: clip_model.transformer.resblocks.2.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([2048])
Parameter: clip_model.transformer.resblocks.2.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([512, 2048])
Parameter: clip_model.transformer.resblocks.2.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.2.ln_2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.2.ln_2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.3.attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: clip_model.transformer.resblocks.3.attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: clip_model.transformer.resblocks.3.attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: clip_model.transformer.resblocks.3.attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.3.ln_1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.3.ln_1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.3.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([2048, 512])
Parameter: clip_model.transformer.resblocks.3.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([2048])
Parameter: clip_model.transformer.resblocks.3.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([512, 2048])
Parameter: clip_model.transformer.resblocks.3.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.3.ln_2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.3.ln_2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.4.attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: clip_model.transformer.resblocks.4.attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: clip_model.transformer.resblocks.4.attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: clip_model.transformer.resblocks.4.attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.4.ln_1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.4.ln_1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.4.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([2048, 512])
Parameter: clip_model.transformer.resblocks.4.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([2048])
Parameter: clip_model.transformer.resblocks.4.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([512, 2048])
Parameter: clip_model.transformer.resblocks.4.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.4.ln_2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.4.ln_2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.5.attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: clip_model.transformer.resblocks.5.attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: clip_model.transformer.resblocks.5.attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: clip_model.transformer.resblocks.5.attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.5.ln_1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.5.ln_1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.5.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([2048, 512])
Parameter: clip_model.transformer.resblocks.5.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([2048])
Parameter: clip_model.transformer.resblocks.5.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([512, 2048])
Parameter: clip_model.transformer.resblocks.5.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.5.ln_2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.5.ln_2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.6.attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: clip_model.transformer.resblocks.6.attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: clip_model.transformer.resblocks.6.attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: clip_model.transformer.resblocks.6.attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.6.ln_1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.6.ln_1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.6.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([2048, 512])
Parameter: clip_model.transformer.resblocks.6.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([2048])
Parameter: clip_model.transformer.resblocks.6.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([512, 2048])
Parameter: clip_model.transformer.resblocks.6.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.6.ln_2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.6.ln_2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.7.attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: clip_model.transformer.resblocks.7.attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: clip_model.transformer.resblocks.7.attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: clip_model.transformer.resblocks.7.attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.7.ln_1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.7.ln_1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.7.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([2048, 512])
Parameter: clip_model.transformer.resblocks.7.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([2048])
Parameter: clip_model.transformer.resblocks.7.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([512, 2048])
Parameter: clip_model.transformer.resblocks.7.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.7.ln_2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.7.ln_2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.8.attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: clip_model.transformer.resblocks.8.attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: clip_model.transformer.resblocks.8.attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: clip_model.transformer.resblocks.8.attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.8.ln_1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.8.ln_1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.8.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([2048, 512])
Parameter: clip_model.transformer.resblocks.8.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([2048])
Parameter: clip_model.transformer.resblocks.8.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([512, 2048])
Parameter: clip_model.transformer.resblocks.8.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.8.ln_2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.8.ln_2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.9.attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: clip_model.transformer.resblocks.9.attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: clip_model.transformer.resblocks.9.attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: clip_model.transformer.resblocks.9.attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.9.ln_1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.9.ln_1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.9.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([2048, 512])
Parameter: clip_model.transformer.resblocks.9.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([2048])
Parameter: clip_model.transformer.resblocks.9.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([512, 2048])
Parameter: clip_model.transformer.resblocks.9.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.9.ln_2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.9.ln_2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.10.attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: clip_model.transformer.resblocks.10.attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: clip_model.transformer.resblocks.10.attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: clip_model.transformer.resblocks.10.attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.10.ln_1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.10.ln_1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.10.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([2048, 512])
Parameter: clip_model.transformer.resblocks.10.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([2048])
Parameter: clip_model.transformer.resblocks.10.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([512, 2048])
Parameter: clip_model.transformer.resblocks.10.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.10.ln_2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.10.ln_2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.11.attn.in_proj_weight, requires_grad: False, shape: torch.Size([1536, 512])
Parameter: clip_model.transformer.resblocks.11.attn.in_proj_bias, requires_grad: False, shape: torch.Size([1536])
Parameter: clip_model.transformer.resblocks.11.attn.out_proj.weight, requires_grad: False, shape: torch.Size([512, 512])
Parameter: clip_model.transformer.resblocks.11.attn.out_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.11.ln_1.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.11.ln_1.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.11.mlp.c_fc.weight, requires_grad: False, shape: torch.Size([2048, 512])
Parameter: clip_model.transformer.resblocks.11.mlp.c_fc.bias, requires_grad: False, shape: torch.Size([2048])
Parameter: clip_model.transformer.resblocks.11.mlp.c_proj.weight, requires_grad: False, shape: torch.Size([512, 2048])
Parameter: clip_model.transformer.resblocks.11.mlp.c_proj.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.11.ln_2.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.transformer.resblocks.11.ln_2.bias, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.token_embedding.weight, requires_grad: False, shape: torch.Size([49408, 512])
Parameter: clip_model.ln_final.weight, requires_grad: False, shape: torch.Size([512])
Parameter: clip_model.ln_final.bias, requires_grad: False, shape: torch.Size([512])
Parameter: output_process.poseFinal.weight, requires_grad: False, shape: torch.Size([263, 512])
Parameter: output_process.poseFinal.bias, requires_grad: False, shape: torch.Size([263])